Current device: cuda:0
normalizing data:
mean: [0.615, 0.4381, 0.645]
std: [0.615, 0.4381, 0.645]
shape of input: torch.Size([256, 3, 256, 256])
Using 2 GPUs...
DataParallel(
  (module): ConvAutoencoder_conv1x1(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv5): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
    (relu): LeakyReLU(negative_slope=0.1)
    (tanh): Tanh()
    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (transconv1): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
    (transconv2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (transconv3): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (transconv4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (transconv5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (sigmoid): Sigmoid()
  )
)
paramters to train:
module.conv1.weight
module.conv1.bias
module.conv2.weight
module.conv2.bias
module.conv3.weight
module.conv3.bias
module.conv4.weight
module.conv4.bias
module.conv5.weight
module.conv5.bias
module.transconv1.weight
module.transconv1.bias
module.transconv2.weight
module.transconv2.bias
module.transconv3.weight
module.transconv3.bias
module.transconv4.weight
module.transconv4.bias
module.transconv5.weight
module.transconv5.bias
 
Epoch 1/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.115301
epoch complete in 13m 35s
 
Epoch 2/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.089316
epoch complete in 13m 25s
 
Epoch 3/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.084108
epoch complete in 13m 44s
 
Epoch 4/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083773
epoch complete in 13m 12s
 
Epoch 5/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083657
epoch complete in 13m 19s
 
Epoch 6/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083575
epoch complete in 13m 23s
 
Epoch 7/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083509
epoch complete in 13m 25s
 
Epoch 8/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083463
epoch complete in 13m 23s
 
Epoch 9/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083426
epoch complete in 13m 30s
 
Epoch 10/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083396
epoch complete in 13m 27s
 
Epoch 11/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083370
epoch complete in 13m 20s
 
Epoch 12/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083212
epoch complete in 13m 25s
 
Epoch 13/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083203
epoch complete in 13m 16s
 
Epoch 14/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083185
epoch complete in 13m 30s
 
Epoch 15/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083168
epoch complete in 13m 31s
 
Epoch 16/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083155
epoch complete in 13m 17s
 
Epoch 17/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083145
epoch complete in 13m 21s
 
Epoch 18/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083136
epoch complete in 13m 20s
 
Epoch 19/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083129
epoch complete in 13m 28s
 
Epoch 20/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083120
epoch complete in 13m 20s
 
Epoch 21/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083115
epoch complete in 13m 39s
 
Epoch 22/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083054
epoch complete in 13m 34s
 
Epoch 23/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083058
epoch complete in 13m 24s
 
Epoch 24/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083053
epoch complete in 13m 14s
 
Epoch 25/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083049
epoch complete in 13m 36s
 
Epoch 26/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083045
epoch complete in 13m 36s
 
Epoch 27/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083041
epoch complete in 13m 24s
 
Epoch 28/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083038
epoch complete in 13m 23s
 
Epoch 29/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083034
epoch complete in 13m 32s
 
Epoch 30/30
---------------
training data on batch 200
training data on batch 400
training data on batch 600
training data on batch 800
training data on batch 1000
training data on batch 1200
Training loss:0.083032
epoch complete in 13m 19s
Best training loss: 0.083032
